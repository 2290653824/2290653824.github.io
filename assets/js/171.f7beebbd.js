(window.webpackJsonp=window.webpackJsonp||[]).push([[171],{494:function(a,e,s){"use strict";s.r(e);var n=s(4),t=Object(n.a)({},(function(){var a=this,e=a._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"kafka-生产者原理分享"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka-生产者原理分享"}},[a._v("#")]),a._v(" "),e("strong",[a._v("kafka 生产者原理分享")])]),a._v(" "),e("h1",{attrs:{id:"一-简介"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-简介"}},[a._v("#")]),a._v(" 一.     "),e("strong",[a._v("简介")])]),a._v(" "),e("h2",{attrs:{id:"_1-什么是kafka"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-什么是kafka"}},[a._v("#")]),a._v(" 1.     "),e("strong",[a._v("什么是kafka")])]),a._v(" "),e("p",[a._v("Kafka是一种分布式的流处理平台，最初由LinkedIn开发。它被设计用来处理大规模的数据流，并具有高吞吐量、可靠性、可扩展性和容错性等特点。Kafka基于发布-订阅模型，通过将消息存储在一个或多个主题中，使得多个消费者可以同时从主题中读取消息并进行处理。Kafka还提供了一些高级功能，如支持消息的持久化、数据压缩和数据分区等。由于其高性能和可靠性，在现代数据架构中被广泛应用于消息队列、日志收集、流处理和事件驱动架构等方面。官网: "),e("a",{attrs:{href:"https://kafka.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[a._v("Apache Kafka"),e("OutboundLink")],1)]),a._v(" "),e("h2",{attrs:{id:"_2-kafka整体架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-kafka整体架构"}},[a._v("#")]),a._v(" 2.     "),e("strong",[a._v("kafka整体架构")])]),a._v(" "),e("p",[a._v("​                ● Broker")]),a._v(" "),e("p",[a._v("服务代理节点。其实就是一个kafka实例或服务节点，多个broker构成了kafka cluster。")]),a._v(" "),e("p",[a._v("​                ● Producer")]),a._v(" "),e("p",[a._v("生产者。也就是写入消息的一方，将消息写入broker中。")]),a._v(" "),e("p",[a._v("​                ● Consumer")]),a._v(" "),e("p",[a._v("消费者。也就是读取消息的一方，从broker中读取消息。")]),a._v(" "),e("p",[a._v("​                ● Consumer Group")]),a._v(" "),e("p",[a._v("消费组。一个或多个消费者构成一个消费组，不同的消费组可以订阅同一个主题的消息且互不影响。")]),a._v(" "),e("p",[a._v("​                ● ZooKeeper")]),a._v(" "),e("p",[a._v("kafka使用zookeeper来管理集群的元数据，以及控制器的选举等操作。")]),a._v(" "),e("p",[a._v("​                ● Topic")]),a._v(" "),e("p",[a._v("主题。每一个消息都属于某个主题，kafka通过主题来划分消息，是一个逻辑上的分类。")]),a._v(" "),e("p",[a._v("​                ● Partition")]),a._v(" "),e("p",[a._v("分区。同一个主题下的消息还可以继续分成多个分区，一个分区只属于一个主题。")]),a._v(" "),e("p",[a._v("​                ● Replica")]),a._v(" "),e("p",[a._v("副本。一个分区可以有多个副本来提高容灾性。")]),a._v(" "),e("p",[a._v("​                ● Leader and Follower")]),a._v(" "),e("p",[a._v("分区有了多个副本，那么就需要有同步方式。kafka使用一主多从进行消息同步，主副本提供读写的能力，而从副本不提供读写，仅仅作为主副本的备份。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng.png",alt:"img"}})]),a._v(" "),e("p",[a._v("以下知识点讲解基于kafka 2.8.0")]),a._v(" "),e("h1",{attrs:{id:"二-消息发送流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-消息发送流程"}},[a._v("#")]),a._v(" 二.     "),e("strong",[a._v("消息发送流程")])]),a._v(" "),e("h2",{attrs:{id:"_1-一条消息发送过程中经历了什么"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-一条消息发送过程中经历了什么"}},[a._v("#")]),a._v(" 1.     "),e("strong",[a._v("一条消息发送过程中经历了什么")])]),a._v(" "),e("p",[a._v("​                ■ KafkaProducer创建一条消息")]),a._v(" "),e("p",[a._v("​                ■ 生产者"),e("strong",[a._v("拦截器")]),a._v("在消息发送之前做一些准备工作，比如过滤不符合要求的消息、修改消息的内容等")]),a._v(" "),e("p",[a._v("​                ■ "),e("strong",[a._v("序列化器")]),a._v("将消息转换成字节数组的形式")]),a._v(" "),e("p",[a._v("​                ■ "),e("strong",[a._v("分区器")]),a._v("计算该消息的目标分区，然后数据会存储在RecordAccumulator中")]),a._v(" "),e("p",[a._v("​                ■ "),e("strong",[a._v("sender线程")]),a._v("获取数据进行发送")]),a._v(" "),e("p",[a._v("​                ■ 创建具体的请求")]),a._v(" "),e("p",[a._v("​                ■ 如果请求过多，会将部分请求缓存起来")]),a._v(" "),e("p",[a._v("​                ■ 将准备好的请求进行发送")]),a._v(" "),e("p",[a._v("​                ■ 发送到kafka集群")]),a._v(" "),e("p",[a._v("​                ■ 接收响应")]),a._v(" "),e("p",[a._v("​                ■ 清理数据")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172722967.png",alt:"img"}})]),a._v(" "),e("h1",{attrs:{id:"三-生产者原理分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三-生产者原理分析"}},[a._v("#")]),a._v(" 三.     "),e("strong",[a._v("生产者原理分析")])]),a._v(" "),e("h2",{attrs:{id:"_1-生产者初始化过程做了什么"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-生产者初始化过程做了什么"}},[a._v("#")]),a._v(" 1.     "),e("strong",[a._v("生产者初始化过程做了什么？")])]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.KafkaProducer#KafkaProducer")]),a._v(" "),e("p",[a._v("1)、设置分区器(partitioner), 分区器是支持自定义的")]),a._v(" "),e("p",[a._v("2)、设置重试时间(retryBackoffMs)默认100ms")]),a._v(" "),e("p",[a._v("3)、设置序列化器(Serializer)")]),a._v(" "),e("p",[a._v("4)、设置拦截器(interceptors)")]),a._v(" "),e("p",[a._v("5)、初始化集群元数据(metadata),刚开始空的")]),a._v(" "),e("p",[a._v("6)、设置最大的消息为多大(maxRequestSize), 默认最大1M")]),a._v(" "),e("p",[a._v("7)、设置缓存大小(totalMemorySize) 默认是32M")]),a._v(" "),e("p",[a._v("8)、设置压缩格式(compressionType)")]),a._v(" "),e("p",[a._v("9)、初始化RecordAccumulator也就是缓冲区指定为32M")]),a._v(" "),e("p",[a._v("10)、定时更新(metadata.update)")]),a._v(" "),e("p",[a._v("11)、创建NetworkClient")]),a._v(" "),e("p",[a._v("12)、创建Sender线程")]),a._v(" "),e("p",[a._v("13)、KafkaThread将Sender设置为守护线程并启动")]),a._v(" "),e("h2",{attrs:{id:"_2-主线程发送消息的过程中做了什么"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-主线程发送消息的过程中做了什么"}},[a._v("#")]),a._v(" 2.     "),e("strong",[a._v("主线程发送消息的过程中做了什么")])]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172723101.png",alt:"img"}})]),a._v(" "),e("h3",{attrs:{id:"重要组件"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#重要组件"}},[a._v("#")]),a._v(" "),e("strong",[a._v("重要组件")])]),a._v(" "),e("h4",{attrs:{id:"_1partitioner-分区器"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1partitioner-分区器"}},[a._v("#")]),a._v(" "),e("strong",[a._v("①partitioner 分区器")])]),a._v(" "),e("p",[a._v("先看看kafka中的partitioner的接口是如何设计的。")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.Partitioner")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public interface Partitioner extends Configurable, Closeable {\n    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); //key可能为null\n}\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("p",[a._v("以为着消息发送到主题的哪个分区，可以由参数中的一个或者多个决定，返回int类型的分区下标。")]),a._v(" "),e("p",[a._v("kafka中实现的分区器有哪些？")]),a._v(" "),e("p",[a._v("​                ● "),e("strong",[a._v("RoundRobinPartitioner  轮询分区器")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n        int nextValue = nextValue(topic);\n        List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);\n        if (!availablePartitions.isEmpty()) {\n            int part = Utils.toPositive(nextValue) % availablePartitions.size();\n            return availablePartitions.get(part).partition();\n        } else {\n            // no partitions are available, give a non-available partition\n            return Utils.toPositive(nextValue) % numPartitions;\n        }\n    }\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br")])]),e("p",[a._v("让每个partition都有分配到消息的机会")]),a._v(" "),e("p",[a._v("​                ● "),e("strong",[a._v("UniformStickyPartitioner 粘性分区器")])]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.UniformStickyPartitioner")]),a._v(" "),e("p",[a._v("什么是粘性分区器？")]),a._v(" "),e("p",[a._v("首先，我们知道，Producer在发送消息的时候,会将消息放到一个ProducerBatch中， 这个Batch可能包含多条消息,然后再将Batch打包发送。这样做的好处就是能够提高吞吐量,减少发起请求的次数。但是有一个问题就是, 因为消息的发送要一个Batch满了或者linger.ms时间到了(当然具体的条件会更多),才会发送。如果生产的消息比较少的话,迟迟难以让Batch塞满，那么就意味着更高的延迟。")]),a._v(" "),e("p",[a._v("在之前的消息发送中,就将消息轮询到各个分区的，本来消息就少,还给所有分区遍历的分配，那么每个ProducerBatch都很难满足条件。粘性分区器的思路就是将一个ProducerBatch塞满之后，再对其他的分区进行分配。")]),a._v(" "),e("p",[a._v("如下图：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172723463.png",alt:"img"}}),e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172723642.png",alt:"img"}})]),a._v(" "),e("p",[a._v("​                ● DefaultPartitioner")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.DefaultPartitioner")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,\n                         int numPartitions) {\n        if (keyBytes == null) {\n            return stickyPartitionCache.partition(topic, cluster);\n        }\n        // hash the keyBytes to choose a partition\n        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n    }\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br")])]),e("p",[a._v("如果消息带key，则对key进行hash后取模对应到partition。如果不带key，则走粘性分区的相关逻辑。")]),a._v(" "),e("p",[a._v("​                ● 自定义分区器")]),a._v(" "),e("p",[a._v("实现接口Partitioner并实现其中的方法，并在初始化Producer时指定对应的分区器：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("Properties props = new Properties();\nprops.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());\nProducer<String, String> producer = new KafkaProducer<>(props);\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("h4",{attrs:{id:"_2recordaccumulator"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2recordaccumulator"}},[a._v("#")]),a._v(" "),e("strong",[a._v("②RecordAccumulator")])]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.RecordAccumulator")]),a._v(" "),e("p",[a._v("kafka性吞吐量更高主要是由于Producer端将多个小消息合并，批量发向Broker。kafka采用异步发送的机制，当发送一条消息时，消息并没有发送到broker而是缓存起来，然后直接向业务返回成功，当缓存的消息达到一定数量时再批量发送。")]),a._v(" "),e("p",[a._v("此时减少了网络io，从而提高了消息发送的性能，但是如果消息发送者宕机，会导致消息丢失，业务出错，所以理论上kafka利用此机制提高了io性能却降低了可靠性。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172724687.png",alt:"img"}})]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public final class RecordAccumulator {\n    private final int batchSize;\n    private final int lingerMs;\n    private final long retryBackoffMs;\n    private final int deliveryTimeoutMs;\n    private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;\n    private final BufferPool free;\n    private final IncompleteBatches incomplete;\n    // The following variables are only accessed by the sender thread, so we don't need to protect them.\n    private final Set<TopicPartition> muted;\n    ……\n}\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br")])]),e("p",[e("strong",[a._v("CopyOnWriteMap")])]),a._v(" "),e("p",[a._v("org.apache.kafka.common.utils.CopyOnWriteMap")]),a._v(" "),e("p",[a._v("kafka自定义CopyOnWriteMap类型，保存了topicPartition与队列的关系。队列里有一个个的小批次，里面是很多消息。这样好处就是可以一次性的把消息发送出去，不至于来一条发送一条，浪费网络资源。")]),a._v(" "),e("p",[a._v("为什么用CopyOnWriteMap类型？可以先看看kafka是如何定义的。")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public class CopyOnWriteMap<K, V> implements ConcurrentMap<K, V> {\n    private volatile Map<K, V> map; //保证线程的可见性\n    \n    public CopyOnWriteMap() {\n        this.map = Collections.emptyMap();\n    }\n    public V get(Object k) { //读操作线程安全\n        return map.get(k);\n    }\n    \n    public synchronized V put(K k, V v) { //更改操作保证线程安全\n        Map<K, V> copy = new HashMap<K, V>(this.map);\n        V prev = copy.put(k, v);\n        this.map = Collections.unmodifiableMap(copy);\n        return prev;\n    }\n    public synchronized V putIfAbsent(K k, V v) {\n        if (!containsKey(k))\n            return put(k, v);\n        else\n            return get(k);\n    }\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br")])]),e("p",[a._v("​\t从上面可以看出来进行put修改操作时，会对当前的map进行复制操作，并在复制的map上进行修改操作。修改期间，其他线程会读取未复制的map，保证了线程的安全。")]),a._v(" "),e("p",[a._v("​\t同时，因为读操作不会加锁，所以CopyOnWriteMap适合读多写少的场景。对于kafka来说，对于读操作，每生产一条消息，都需要从这个map中读取；对于写操作，假设有10个分区，就会向这个map中插入10条数据。综上来看，读操作是远远大于写操作的。")]),a._v(" "),e("p",[a._v("可见CopyOnWriteMap这个结构在高并发下是线程安全的")]),a._v(" "),e("p",[a._v("但由此也带来了问题，生产者端消息这么多，一个批次发送完了就不管了去等待 JVM 的垃圾回收的时候，很有可能会触发 full gc。一次 full gc，整个 Producer 端的所有线程就都停了，所有消息都无法发送了，由此带来的损耗也是不可小觑。")]),a._v(" "),e("p",[a._v("kafka设计了内存池，用来反复利用被发送出去 RecordBatch，以减少 full gc。")]),a._v(" "),e("p",[e("strong",[a._v("bufferPool")])]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172724931.png",alt:"img"}})]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public class BufferPool {\n    private final long totalMemory;\n    private final int poolableSize;\n    private final ReentrantLock lock;\n    private final Deque<ByteBuffer> free;\n    private final Deque<Condition> waiters;\n    /** Total available memory is the sum of nonPooledAvailableMemory and the number of byte buffers in free * poolableSize.  */\n    private long nonPooledAvailableMemory;\n    ……\n}\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br")])]),e("p",[a._v("当CopyOnWriteMap的队列中的ProducerBatch不够时，会向bufferPool申请新的ProducerBatch空间。当ProducerBatch发送成功后，会向bufferPool归还空间。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172725031.png",alt:"img"}})]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.BufferPool#allocate")]),a._v(" "),e("p",[a._v("可以看出，就算是使用异步的发送方式，如果没有设置好缓存大小的话，也是会出现阻塞的。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172725164.png",alt:"img"}})]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.BufferPool#deallocate(java.nio.ByteBuffer, int)")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v(" public void deallocate(ByteBuffer buffer, int size) {\n        lock.lock();\n        try {\n            if (size == this.poolableSize && size == buffer.capacity()) {\n                buffer.clear();\n                this.free.add(buffer);\n            } else {\n                this.nonPooledAvailableMemory += size;\n            }\n            Condition moreMem = this.waiters.peekFirst();\n            if (moreMem != null)\n                moreMem.signal();\n        } finally {\n            lock.unlock();\n        }\n    }\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br")])]),e("p",[a._v("从deallocate可以看出：如果要释放的batch的size大于poolableSize的话，是不会进入free进行内存重复利用的，反而是等待 垃圾回收器来进行回收。如果因为batch.size设置不当，则会导致频繁的GC。")]),a._v(" "),e("h2",{attrs:{id:"_3-senderthread-线程做了什么"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-senderthread-线程做了什么"}},[a._v("#")]),a._v(" 3.     "),e("strong",[a._v("SenderThread 线程做了什么")])]),a._v(" "),e("p",[a._v("重要的成员")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public class Sender implements Runnable {\n    private final KafkaClient client; // 为 Sender 线程提供管理网络连接进行网络读写\n    private final RecordAccumulator accumulator; // 消息累加器\n    private final ProducerMetadata metadata; // 生产者元数据\n    private final int maxRequestSize; //发送消息最大字节数。  \n    private final short acks; // 生产者的消息发送确认机制\n    private final int retries; // 发送失败后的重试次数，默认为0次\n\n    private volatile boolean running; // Sender 线程是否还在运行中     \n    private volatile boolean forceClose; // 是否强制关闭，此时会忽略正在发送中的消息。\n    private final int requestTimeoutMs; // 等待服务端响应的最大时间,默认30s       \n    private final long retryBackoffMs; // 失败重试退避时间      \n    private final ApiVersions apiVersions; // 所有 node 支持的 api 版本\n    private final Map<TopicPartition, List<ProducerBatch>> inFlightBatches; // 正在执行发送相关的消息批次集合， key为分区，value是 list<ProducerBatch> 。\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br")])]),e("p",[a._v("Sender 线程实现了 Runnable 接口，会不断的调用 runOnce()，这是一个典型的循环事件机制。")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('public void run() {\n     ……\n        while (running) {\n            try {\n                runOnce();\n            } catch (Exception e) {\n                log.error("Uncaught error in kafka producer I/O thread: ", e);\n            }\n        }\n    ……\n}\n\nvoid runOnce() {\n         ……\n        // 1. 获取当前时间的时间戳。\n        long currentTimeMs = time.milliseconds();\n        // 2. 调用 sendProducerData 发送消息,但并非真正的发送，而是把消息缓存在 把消息缓存在inflightBatches中\n       long pollTimeout = sendProducerData(currentTimeMs);\n        // 3. 读取消息实现真正的网络发送\n        client.poll(pollTimeout, currentTimeMs);\n}\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br")])]),e("h3",{attrs:{id:"sender线程整体流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sender线程整体流程"}},[a._v("#")]),a._v(" "),e("strong",[a._v("sender线程整体流程")])]),a._v(" "),e("p",[a._v("​            \\1.     首先获取元数据，主要是根据元数据的更新机制来保证数据的准确性。")]),a._v(" "),e("p",[a._v("​            \\2.     获取已经准备好的节点。这里会遍历accumulate中的batches，并找出满足发送条件的batch，并统计其所在partition的leader所在node节点。返回集合readyNodes")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.RecordAccumulator#ready")]),a._v(" "),e("p",[a._v("​            \\3.     如果主题 Leader 分区对应的节点不存在，则强制更新元数据。")]),a._v(" "),e("p",[a._v("​            \\4.     循环 readyNodes 并检查客户端与要发送节点的网络是否已经建立好了。在 NetworkClient 中维护了客户端与所有节点的连接，这样就可以通过连接的状态判断是否连接正常。")]),a._v(" "),e("p",[a._v("怎么检查一个node是否有没有与客户端建立好连接？")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.NetworkClient#ready")]),a._v(" "),e("p",[a._v("​            \\5.     获取上面返回的已经准备好的节点上要发送的 ProducerBatch 集合。accumulator#drain() 方法就是将 「TopicPartition」-> 「ProducerBatch 集合」的映射关系转换成 「Node 节点」->「ProducerBatch 集合」的映射关系，如下图所示，这样的话按照节点方式只需要2次就完成，大大减少网络的开销。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172725662.png",alt:"img"}})]),a._v(" "),e("p",[a._v("Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本＜分区，Deque＜ProducerBatch＞＞的保存形式转变成＜Node，List＜ ProducerBatch＞的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I/O层面的转换。")]),a._v(" "),e("p",[a._v("​            \\6.     将从消息累加器中读取的数据集，放入正在执行发送相关的消息批次集合中inFlightBatches。")]),a._v(" "),e("p",[a._v("​            \\7.     发送消息暂存到 NetworkClient inflightRequests 里。inflightRequests 对已经被发送或正在被发送但是均未接收到响应的客户端请求集合的一个封装。")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.Sender#sendProduceRequest")]),a._v(" "),e("h4",{attrs:{id:"什么是满足发送要求的batch"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#什么是满足发送要求的batch"}},[a._v("#")]),a._v(" "),e("strong",[a._v("什么是满足发送要求的batch")])]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.RecordAccumulator#ready")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("boolean full = deque.size() > 1 || batch.isFull();\nboolean expired = waitedTimeMs >= timeToWaitMs;\nboolean sendable = full || expired || exhausted || closed || flushInProgress();\nif (sendable && !backingOff) {\n    readyNodes.add(leader);\n} \n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br")])]),e("h3",{attrs:{id:"生产者如何进行消息重试"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#生产者如何进行消息重试"}},[a._v("#")]),a._v(" "),e("strong",[a._v("生产者如何进行消息重试")])]),a._v(" "),e("p",[a._v("sendProducerData ->sendProduceRequests ->handleProduceResponse->completeBatch->canRetry")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.Sender#handleProduceResponse")]),a._v(" "),e("p",[a._v("在消息发送完成后，producer会接受到broker的响应信息，handleProduceResponse 就用于对响应的消息进行处理")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.Sender#canRetry")]),a._v(" "),e("p",[a._v("通过源码来看，如果返回的错误是 Errors.NONE 错误，则可以进行进行是否消息可重试的检测")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("private boolean canRetry(ProducerBatch batch, ProduceResponse.PartitionResponse response, long now) {\n        return !batch.hasReachedDeliveryTimeout(accumulator.getDeliveryTimeoutMs(), now) &&\n            batch.attempts() < this.retries &&\n            !batch.isDone() &&\n            (transactionManager == null ?\n                    response.error.exception() instanceof RetriableException :\n                    transactionManager.canRetry(response, batch));\n    }\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br")])]),e("p",[a._v("可以进行重试的条件：")]),a._v(" "),e("p",[a._v("​                ● 重试的次数,这个就是我们的客户端的配置")]),a._v(" "),e("p",[a._v("​                ● 异常是不是可重试异常response.error.exception() instanceof RetriableException)，下面就是全部的可重试异常")]),a._v(" "),e("p",[a._v("org.apache.kafka.common.errors.RetriableException")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172725839.png",alt:"img"}})]),a._v(" "),e("p",[a._v("如果发现batch是可以被重试的，则将这个batch重新加到RecordAccumulate累加器中。需要注意的是重试添加的batch是添加在duque的头部，而主线程发送消息是添加在batch的尾部。")]),a._v(" "),e("p",[a._v("加到deque的头部后，sender什么时候对这个重试的batch进行发送呢？")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.RecordAccumulator#ready")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("long waitedTimeMs = batch.waitedTimeMs(nowMs);\nboolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;\nlong timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;\nboolean full = deque.size() > 1 || batch.isFull();\nboolean expired = waitedTimeMs >= timeToWaitMs;\nboolean sendable = full || expired || exhausted || closed || flushInProgress();\nif (sendable && !backingOff) {\n    readyNodes.add(leader);\n} \n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br")])]),e("p",[a._v("从上面的代码可以看出，重试的代码并没有进行 partitioner 分区器的重新分配，重试的消息会发送到相同的分区。保证了幂等性")]),a._v(" "),e("h3",{attrs:{id:"生产者callback机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#生产者callback机制"}},[a._v("#")]),a._v(" "),e("strong",[a._v("生产者callback机制")])]),a._v(" "),e("p",[a._v("发送一条消息的流程")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('String value = " this is another message_" + i;\n            ProducerRecord<String,String> record = new ProducerRecord<String, String>(topic,i+"",value);\n            procuder.send(record,new Callback() {\n                @Override\n                public void onCompletion(RecordMetadata metadata, Exception exception) {\n                        System.out.println("message send to partition " + metadata.partition() + ", offset: " + metadata.offset());\n                }\n            });\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br")])]),e("p",[a._v("CallBack.class")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("public interface Callback {\n   void onCompletion(RecordMetadata metadata, Exception exception);\n}\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("p",[a._v("CallBack只有一个onCompletion方法，传入两个参数metadata和exception。")]),a._v(" "),e("p",[a._v("那么在一个消息的发送过程中，他的一个子实现对象的是如何进行保存，又如何进行调用的呢？")]),a._v(" "),e("p",[a._v("首先，用户自定义的Callback对象会在org.apache.kafka.clients.producer.KafkaProducer#doSend 中进行一次封装")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("// producer callback will make sure to call both 'callback' and interceptor callback\n            Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("p",[a._v("封装后可以在回调时同时调用拦截器的onAcknowledgement方法和用户自定义的onCompletion方法")]),a._v(" "),e("p",[a._v("后续主线程将信息追加到RecordAccumate中的时候，会将interceptCallback 添加到ProducerBatch中：")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.ProducerBatch#tryAppend")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("thunks.add(new Thunk(callback, future));\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("至此，主线程对回调函数的干预就到此为止了。")]),a._v(" "),e("p",[a._v("以上其实是对batch中每个record的回调函数的讲解.后续batch会被封装为ClientRequest对象存放inflightRequests中。在其实对于每个ClientRequest本身也会存在一个回调函数。具体代码位置为：")]),a._v(" "),e("p",[a._v("org.apache.kafka.clients.producer.internals.Sender#sendProduceRequest")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("RequestCompletionHandler callback = response -> handleProduceResponse(response, recordsByPartition, time.milliseconds());\n\n        String nodeId = Integer.toString(destination);\n        ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,\n                requestTimeoutMs, callback);\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br")])]),e("p",[a._v("为什么ClientRequest 也会需要一个回调函数？因为producer与broker的交互最终是落实到与clientRequest进行交互的，最终进行回调的时候，broker会返回给producer clientResponse，通过clientResponse中的回调函数就可以进行相应的函数调用:")]),a._v(" "),e("p",[a._v("具体的回调函数可以从代码 org.apache.kafka.clients.NetworkClient#poll 开始")]),a._v(" "),e("p",[a._v("poll -> completeResponses ->  clientResponse.onComplete ->  handleProduceResponse ->  completeBatch -> done -> completeFutureAndFireCallbacks")]),a._v(" "),e("h1",{attrs:{id:"四-总结"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#四-总结"}},[a._v("#")]),a._v(" 四.     "),e("strong",[a._v("总结")])]),a._v(" "),e("p",[e("img",{attrs:{src:"https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/undefinedpng-20230509172723101.png",alt:"img"}})]),a._v(" "),e("h2",{attrs:{id:""}},[e("a",{staticClass:"header-anchor",attrs:{href:"#"}},[a._v("#")])]),a._v(" "),e("p",[a._v("参考:")]),a._v(" "),e("p",[a._v("​                ● "),e("a",{attrs:{href:"https://kafka.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[a._v("Apache Kafka"),e("OutboundLink")],1)]),a._v(" "),e("p",[a._v("​                ● 《深入理解kafka核心设计与实践原理》")]),a._v(" "),e("p",[a._v("​                ● "),e("a",{attrs:{href:"https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzg3MTcxMDgxNA==&action=getalbum&album_id=2147575846151290880&scene=173&from_msgid=2247488849&from_itemidx=1&count=3&nolastread=1#wechat_redirect",target:"_blank",rel:"noopener noreferrer"}},[a._v("华仔聊技术"),e("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=t.exports}}]);